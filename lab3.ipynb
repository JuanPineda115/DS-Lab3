{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Martín Amado - 19020\n",
    "Juan Pablo Pineda - 19087\n",
    "\n",
    "Referencia para análisis de datos: https://www.kaggle.com/competitions/digit-recognizer/data?select=test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.image as mpimg\r\n",
    "import seaborn as sns\r\n",
    "import numpy as np\r\n",
    "import itertools\r\n",
    "import matplotlib.cm as cm\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import confusion_matrix\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Convolution2D, MaxPooling2D\r\n",
    "from keras.layers import LSTM\r\n",
    "from scipy import stats\r\n",
    "from factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "from keras.utils.np_utils import to_categorical\r\n",
    "from sklearn.pipeline import make_pipeline\r\n",
    "from keras.optimizers import RMSprop\r\n",
    "from keras.preprocessing.image import ImageDataGenerator\r\n",
    "from keras.callbacks import ReduceLROnPlateau\r\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\r\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\r\n",
    "from statsmodels.tsa.stattools import adfuller\r\n",
    "from apyori import apriori\r\n",
    "\r\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\r\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitRec_train = pd.read_csv(\"./train.csv\")\n",
    "digitRec_test = pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitRec_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitRec_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis Exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseMatrix = digitRec_train.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.new('RGB', (28, 28), \"black\")\n",
    "numbersMatrices = []\n",
    "\n",
    "for x in baseMatrix[:5]:\n",
    "    localmat = []\n",
    "    for pixel in range(1, len(x), 28):\n",
    "        localmat.append(list(x[pixel:pixel+28]))\n",
    "    for row in range(0,28):\n",
    "        for col in range(0,28):\n",
    "            img.putpixel((col,row), (localmat[row][col], localmat[row][col], localmat[row][col]))\n",
    "    numbersMatrices.append(localmat)\n",
    "    img.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-procesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = digitRec_train['label']\n",
    "X_train = digitRec_train.drop(['label'], axis=1)\n",
    "\n",
    "g = sns.countplot(Y_train)\n",
    "\n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitRec_test.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "digitRec_test = digitRec_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "digitRec_test = digitRec_test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(Y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 2\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer Modelo de Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='Same',\n",
    "                 activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='Same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size = 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                              epochs=epochs, validation_data=(X_val, Y_val),\n",
    "                              verbose=2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = X_val[errors]\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(digitRec_test)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitRec_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caracteristicas\r\n",
    "- Filtros: 128\r\n",
    "- Detector de propiedades de 3x3\r\n",
    "- Movimiento de pixel en pixel\r\n",
    "- Tamano imagenes 28x28, Lightness\r\n",
    "- Matriz baja a 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Sequential()\r\n",
    "modelo.add(Convolution2D(128,(3,3),strides=(1,1), input_shape=(28,28,1), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segunda pasada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.add(Convolution2D(64, (3, 3), strides=(1, 1), activation='relu'))\r\n",
    "modelo.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flattening y Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.add(Flatten())\r\n",
    "modelo.add(Dense(256, activation='relu'))\r\n",
    "modelo.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer='adam', loss='categorical_hinge', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\r\n",
    "batch = 32\r\n",
    "## Se vuelve a importar para evitar cambios realizados\r\n",
    "digitRec_train = pd.read_csv(\"./train.csv\")\r\n",
    "digitRec_test = pd.read_csv(\"./test.csv\")\r\n",
    "\r\n",
    "Y_train = digitRec_train['label']\r\n",
    "X_train = digitRec_train.drop(['label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\r\n",
    "digitRec_test = digitRec_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values.reshape(-1,28,28,1)\r\n",
    "digitRec_test = digitRec_test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(Y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 5\r\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\r\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\r\n",
    "        samplewise_center=False,  # set each sample mean to 0\r\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\r\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\r\n",
    "        zca_whitening=False,  # apply ZCA whitening\r\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\r\n",
    "        zoom_range = 0.1, # Randomly zoom image \r\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\r\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\r\n",
    "        horizontal_flip=False,  # randomly flip images\r\n",
    "        vertical_flip=False)  # randomly flip images\r\n",
    "\r\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = modelo.fit(\r\n",
    "  X_train,\r\n",
    "  epochs=epochs\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Series de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumos Diesel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recaudacion y limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption = pd.read_excel('./CONSUMO-2022-05.xlsx', skiprows=6)\n",
    "consumption = consumption[['Fecha', 'Diesel']]\n",
    "#omitimos los datos despues del 269 ya que no aportan relevancia\n",
    "consumption = consumption[:269]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desarrollo de la seria de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSize = int(len(consumption) * 0.7)\n",
    "trainConsumos = consumption[0:trainSize]\n",
    "testConsumos = consumption[trainSize:len(consumption)]\n",
    "trainConsumos = trainConsumos.set_index(['Fecha'])\n",
    "testConsumos = testConsumos.set_index(['Fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsDiesel = trainConsumos['Diesel']\n",
    "mediaMovil = tsDiesel.rolling(window=12).mean()\n",
    "deMovil = tsDiesel.rolling(window=12).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = plt.plot(tsDiesel, color=\"blue\", label=\"Original\")\n",
    "media = plt.plot(mediaMovil, color='red', label = 'Media Movil')\n",
    "ds = plt.plot(deMovil,color='black', label = 'Desviación Estándar Móvil')\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Media y desviación estándar móvil')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descomposicion = seasonal_decompose(tsDiesel)\n",
    "descomposicion.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsDiesel = tsDiesel.astype({'Diesel':'float'})\n",
    "tsDieselLog = np.log(tsDiesel)\n",
    "plt.plot(tsDieselLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = adfuller(tsDiesel, autolag='AIC')\n",
    "salidaDf = pd.Series(dfTest[0:4], index=['Estadístico de prueba','p-value','# de retardos usados','# de observaciones usadas'])\n",
    "for key,value in dfTest[4].items():\n",
    "        salidaDf['Valor Crítico (%s)'%key] = value\n",
    "print(salidaDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsDieselDiff = tsDiesel.diff()\n",
    "tsDieselDiff.fillna(0,inplace=True)\n",
    "dfTest = adfuller(tsDieselDiff)\n",
    "salidaDf = pd.Series(dfTest[0:4], index=['Estadístico de prueba','p-value','# de retardos usados','# de observaciones usadas'])\n",
    "for key,value in dfTest[4].items():\n",
    "        salidaDf['Critical Value (%s)'%key] = value\n",
    "salidaDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tsDieselDiff)\n",
    "plt.title('Diferenciacion de la serie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo111 = SARIMAX(tsDieselLog, order=(1,1,1), seasonal_order=(3,1,0,12), enforce_stationarity=False, enforce_invertibility=False)\n",
    "resultado_m111 = modelo111.fit()\n",
    "resultado_m111.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_m111.plot_diagnostics(figsize=(18, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testConsumos.index[0]\n",
    "pred = resultado_m111.get_prediction(start=testConsumos.index[0], dynamic=False)\n",
    "pred_ci = pred.conf_int()\n",
    "consumoIndexed = consumption.set_index('Fecha')\n",
    "consumoIndexed = consumoIndexed['Diesel']\n",
    "ax = consumoIndexed['2000':].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 4))\n",
    "ax.fill_between( pred_ci.iloc[:,0],\n",
    "                pred_ci.iloc[:,1], color='k', alpha=.2)\n",
    "#ax.set_xlabel('Date')\n",
    "#ax.set_ylabel('Retail_sold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumo = pd.read_excel('./CONSUMO-2022-05.xlsx', skiprows=6)\n",
    "consumo = consumo[['Fecha', 'Diesel']]\n",
    "#omitimos los datos despues del 269 ya que no aportan relevancia\n",
    "consumo = consumo[:269]\n",
    "consumo = consumo.set_index('Fecha')\n",
    "consumo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(consumo)\n",
    "plt.gca().set(title=\"Consumo de Diesel por mes por año\", xlabel=\"Fecha\", ylabel=\"Consumo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estacionarizar para uso de LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula la media móvil y la desviación estandar móvil de los últimos 12 meses.\n",
    "mediaMovil = consumo.rolling(window=12).mean()\n",
    "deMovil = consumo.rolling(window=12).std()\n",
    "# Se grafican los resultados.\n",
    "original = plt.plot(consumo, color=\"blue\", label=\"Original\")\n",
    "media = plt.plot(mediaMovil, color='red', label = 'Media Movil')\n",
    "ds = plt.plot(deMovil,color='black', label = 'Desviación Estándar Móvil')\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Media y desviación estándar móvil')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descomposicion = seasonal_decompose(consumo)\n",
    "descomposicion.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe tendencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resultados del Test de Dickey Fuller')\n",
    "dfTest = adfuller(consumo, autolag='AIC')\n",
    "salidaDf = pd.Series(dfTest[0:4], index=['Estadístico de prueba','p-value','# de retardos usados','# de observaciones usadas'])\n",
    "for key,value in dfTest[4].items():\n",
    "        salidaDf['Critical Value (%s)'%key] = value\n",
    "print(salidaDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resultados del Test de Dickey Fuller para una diferenciación de la serie')\n",
    "diesel_diff = consumo.diff()\n",
    "diesel_diff.fillna(0,inplace=True)\n",
    "dfTest = adfuller(diesel_diff)\n",
    "salidaDf = pd.Series(dfTest[0:4], index=['Estadístico de prueba','p-value','# de retardos usados','# de observaciones usadas'])\n",
    "for key,value in dfTest[4].items():\n",
    "        salidaDf['Critical Value (%s)'%key] = value\n",
    "print(salidaDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(diesel_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesidad de diferenciacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "diesel_scaled = scaler.fit_transform(diesel_diff) \n",
    "diesel_scaled[1:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60% entrenamiento, 20% validacion y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diesel = round(0.6*len(diesel_scaled))\n",
    "val_test_diesel = round(0.2*len(diesel_scaled))\n",
    "test = diesel_scaled[(train_diesel + val_test_diesel) - 1:]\n",
    "validation = diesel_scaled[(train_diesel):train_diesel + val_test_diesel + 1]\n",
    "train = diesel_scaled[0:train_diesel]\n",
    "train = np.insert(train, 0, 0)\n",
    "train = np.reshape(train, (train.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformacion a serie supervisada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import concat\n",
    "\n",
    "\n",
    "def supervisada(serie,retrasos = 1):\n",
    "    serie_x = []\n",
    "    serie_y = []\n",
    "    for i in range(len(serie)-retrasos):\n",
    "        valor = serie[i:(i+retrasos),0]\n",
    "        valor_sig = serie[i+retrasos,0]\n",
    "        serie_x.append(valor)\n",
    "        serie_y.append(valor_sig)\n",
    "    return np.array(serie_x), np.array(serie_y)\n",
    "\n",
    "x_train,y_train = supervisada(train)\n",
    "x_val,y_val = supervisada(validation)\n",
    "x_test,y_test = supervisada(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices de 3 dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train,(x_train.shape[0],1,1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0],1,1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo con una capa LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo1 = Sequential()\n",
    "lote = 1\n",
    "unidades =  1\n",
    "paso = 1\n",
    "caracteristicas = 1 #es univariada\n",
    "modelo1.add(LSTM(lote, batch_input_shape=(lote,paso,caracteristicas),stateful=True))\n",
    "modelo1.add(Dense(1))\n",
    "modelo1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo1.compile(loss='mean_squared_error',optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas = 50\n",
    "history= modelo1.fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    batch_size = lote,\n",
    "    epochs = epocas,\n",
    "    shuffle = False,\n",
    "    validation_data = (x_val,y_val),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pérdida en Entrenamiento\")\n",
    "modelo1.evaluate(\n",
    "    x = x_train,\n",
    "    y = y_train\n",
    ")\n",
    "print(\"Pérdida en Validación\")\n",
    "modelo1.evaluate(\n",
    "    x = x_val,\n",
    "    y = y_val\n",
    ")\n",
    "print(\"Pérdida en Prueba\")\n",
    "modelo1.evaluate(\n",
    "    x = x_test,\n",
    "    y = y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediccion modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_val = []\n",
    "\n",
    "def prediccion_fun(data,modelo, batch_size,scaler,dif=False,dif_cant=1, Series = None , n=1):\n",
    "    prediccion = [0]* (len(data))\n",
    "    i=0\n",
    "    for X in data:\n",
    "        X = np.reshape(X,(1,1,1))\n",
    "        yhat = modelo1.predict(X, batch_size=batch_size,verbose=0)\n",
    "        # invert scaling\n",
    "        yhat = scaler.inverse_transform(yhat)\n",
    "        if dif:\n",
    "             # invert differencing\n",
    "            yhat  = yhat + Series[(n+dif_cant*i)]\n",
    "        # store\n",
    "        prediccion[i]=yhat[0][0]\n",
    "        i = i+1\n",
    "    return prediccion\n",
    "prediccion_val = prediccion_fun(x_val,modelo1, 1,scaler,dif=True,dif_cant=1, Series = consumo.values , n=train_diesel)\n",
    "prediccion_test = prediccion_fun(x_test,modelo1, 1,scaler,dif=True,dif_cant=1, Series = consumo.values , n=train_diesel+val_test_diesel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.DataFrame(prediccion_val,index=consumo[(train_diesel):train_diesel+val_test_diesel].index)\n",
    "df_test = pd.DataFrame(prediccion_test,index=consumo[train_diesel+len(df_val):len(consumo)].index)\n",
    "\n",
    "\n",
    "plt.plot(consumo)\n",
    "plt.plot(df_val)\n",
    "plt.plot(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo 2, las unidades cambiaran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo2 = Sequential()\n",
    "lote = 1\n",
    "unidades =  5\n",
    "paso = 1\n",
    "caracteristicas = 1 #es univariada\n",
    "modelo2.add(LSTM(lote, batch_input_shape=(lote,paso,caracteristicas),stateful=True))\n",
    "modelo2.add(Dense(1))\n",
    "modelo2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo2.compile(loss='mean_squared_error',optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas = 50\n",
    "history= modelo2.fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    batch_size = lote,\n",
    "    epochs = epocas,\n",
    "    shuffle = False,\n",
    "    validation_data = (x_val,y_val),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pérdida en Entrenamiento\")\n",
    "modelo2.evaluate(\n",
    "    x = x_train,\n",
    "    y = y_train\n",
    ")\n",
    "print(\"Pérdida en Validación\")\n",
    "modelo2.evaluate(\n",
    "    x = x_val,\n",
    "    y = y_val\n",
    ")\n",
    "print(\"Pérdida en Prueba\")\n",
    "modelo2.evaluate(\n",
    "    x = x_test,\n",
    "    y = y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_val = []\n",
    "\n",
    "def prediccion_fun(data,modelo, batch_size,scaler,dif=False,dif_cant=1, Series = None , n=1):\n",
    "    prediccion = [0]* (len(data))\n",
    "    i=0\n",
    "    for X in data:\n",
    "        X = np.reshape(X,(1,1,1))\n",
    "        yhat = modelo1.predict(X, batch_size=batch_size,verbose=0)\n",
    "        # invert scaling\n",
    "        yhat = scaler.inverse_transform(yhat)\n",
    "        if dif:\n",
    "             # invert differencing\n",
    "            yhat  = yhat + Series[(n+dif_cant*i)]\n",
    "        # store\n",
    "        prediccion[i]=yhat[0][0]\n",
    "        i = i+1\n",
    "    return prediccion\n",
    "prediccion_val = prediccion_fun(x_val,modelo1, 1,scaler,dif=True,dif_cant=1, Series = consumo.values , n=train_diesel)\n",
    "prediccion_test = prediccion_fun(x_test,modelo1, 1,scaler,dif=True,dif_cant=1, Series = consumo.values , n=train_diesel+val_test_diesel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.DataFrame(prediccion_val,index=consumo[(train_diesel):train_diesel+val_test_diesel].index)\n",
    "df_test = pd.DataFrame(prediccion_test,index=consumo[train_diesel+len(df_val):len(consumo)].index)\n",
    "\n",
    "\n",
    "plt.plot(consumo)\n",
    "plt.plot(df_val)\n",
    "plt.plot(df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusiones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos LSTM fueron de mayor utilidad, pues presentaron una prediccion, a diferencia del utilizado anteriormente.\r\n",
    "\r\n",
    "El segundo modelo LSTM es mejor, debido a los valores de perdida que presenta en entrenamiento validacion y prueba"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "6cb11118430048aaffde37140829a96b2f2778371a079ecf7cef1d9eee249f62"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1fbb46e97979fe274de0b1708aa8583f7166e6b85cafc938966f13b05b745da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}